<html>
<head>
<title>CS 479 Final Project</title>
<link href='http://fonts.googleapis.com/css?family=Nunito:300|Crimson+Text|Droid+Sans+Mono' rel='stylesheet' type='text/css'>
<link rel="stylesheet" title="Default" href="styles/github.css">
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script>  

<link rel="stylesheet" href="highlighting/styles/default.css">
<script src="highlighting/highlight.pack.js"></script>

<style type="text/css">
body {
	margin: 0px;
	width: 100%;
	font-family: 'Crimson Text', serif;
	font-size: 20px;
	background: #fcfcfc;
}
h1 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 28px;
	margin: 25px 0px 0px 0px;
}

h2 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 32px;
	margin: 15px 0px 35px 0px;
	color: #333;	
	word-spacing: 3px;
}

h3 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 26px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}
h4 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 22px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}

h5 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 18px;
	margin: 10px 0px 10px 0px;
	color: #111;
	word-spacing: 2px;
}

p, li {
	color: #444;
}

a {
	color: #DE3737;
}

.container {
	margin: 0px auto 0px auto;
	width: 1160px;
}

#header {
	background: #333;
	width: 100%;
}

#headersub {
	color: #ccc;
	width: 960px;
	margin: 0px auto 0px auto;
	padding: 20px 0px 20px 0px;
}

.chart {
	width: 480px;
}
.lol {
	font-size: 16px;
	color: #888;
	font-style: italic;
}
.sep {
	height: 1px;
	width: 100%;
	background: #999;
	margin: 20px 0px 20px 0px;
}
.footer{
	font-size: 16px;
}
.latex {
	width: 100%;
}

.latex img {
	display: block;
	margin: 0px auto 0px auto;
}

pre {
	font-family: 'Droid Sans Mono';
	font-size: 14px;
}

table td {
  text-align: center;
  vertical-align: middle;
}

table td img {
  text-align: center;
  vertical-align: middle;
}

#contents a {
}
</style>
<script type="text/javascript">
    hljs.initHighlightingOnLoad();
</script>
</head>
<body>
<div id="header" >
<div id="headersub">
<h1>The Active Learners&nbsp;&nbsp; <span style="color: #DE3737">(Jason Hubbs, Cole Nicholson-Rubidoux, Christopher Roseberry, Aidan Roth)</span></h1>
</div>
</div>
<div class="container">

<h2>CS 479 : Animal Detection with Active Learning</h2>

<div style="float: right; padding: 20px">
<img src="mult_image_detections.jpg" />
<p style="font-size: 14px">Detection of a deer and a turkey.</p>
</div>

<p> 	The focus of our project was animal detection for the purpose of tracking biodiversity via images captured by camera traps in a nature reserve. Our dataset comes from the Fairfield Osbourne Preserve here in Sonoma County, California and consists of roughly 25,000 labeled images of the wildlife that lives there. The core problem we were trying to solve was the efficiency of animal labeling in machine learning. Manually labeling images is extremely slow and takes away from the limited resources that ecologist have. While standard machine learning methods have greatly improved this process, they require a large pre-existing training set that must be well labeled. To combat this limitation and further improve machine learning in image labeling, we implemented the Active Learning algorithm which requires fewer labels and can ideally improve accuracy. The essential milestones of our project are as follows:</p>

<ol>
<li>Run dataset through object detection script and crop out areas of interest.</li>
<li>Group images by isolated camera events to prevent bias.</li>
<li>Implement oversampling to make up for imbalanced animal categories.</li>
<li>Train machine learning model on improved dataset.</li>
<li>Implement k-Fold and Leave-One-Camera-Out cross-validation to estimate accuracy.</li>
<li>Build and utilize Active Learning algorithm with trained model.</li>
</ol>

<p> 	Applying Active Learning to our classification model provided us with mixed results. Our Active Learning network peaked around 76% accuracy and f_scores remained low. However, this was in part due to the quality of our dataset. The number of images for each animal varied greatly, with the number of images containing no animals being more than twice the size of the next largest category, deer. There were also numerous mislabeled images, which mistrained the classifier and made analyzing results more difficult. </p>

<div style="clear:both">
<h3>Building the Classification Model</h3>

<p> 	The first major goal in our project was building a machine learning model that could classify different types of animals. The initial step involved in this was running our dataset through the CameraTrap animal detection script built my Microsoft AI for Earth which can be found <a href="https://github.com/microsoft/CameraTraps" target="_blank">here</a>. This Python script provided us with a JSON file that contained the coordinates of each animal it detected in each image we gave it. We then used MatLab to parse that file and crop down the images to feature the animals as clearly as possible. A visual representation of what the detector does is shown below, along side what its equivalent JSON output looks like. (Full JSON file can be found <a href="report_output.json" target="_blank">here</a>) </p>
	
<div class="row">
  <div class="column">
    <img src="mult_image_detections.jpg" alt="boxes" style="width:100%">
  </div>
  <div class="column">
    <img src="alt_small_JSON_screenshot.png" alt="JSON" style="width:100%">
  </div>
</div>	
	
<p>The next step involved grouping together specific images. The camera traps that created our dataset are set up in such a way that, when they detect movement, they take a burst of three pictures. Together, these three images are single event. In order to prevent bias in our classification network, we needed to make sure that each event had all three of its images in either the training set or the test set. We wrote a Python script to group the images by event and prevent bias from occurring in this way.</p>
	
<p>The third step in improving our dataset was oversampling. This process involves duplicating the images in the categories with fewer images until they equal the number of images in the largest category. Due to the extreme imbalance in the categories of our dataset, this resulted in some images being duplicated thousands of times. For example, there were only five pictures of hawks in our dataset, compared with 4,272 deer pictures. The oversampling technique we used in MatLab duplicated those five hawk images over and over again. While this did help our results somewhat, oversampling seems to be more effective when the imbalance of categories is less extreme. </p>
	
<p>Once we had refined and improved our dataset as much as possible, we used it to train a classification model and implemented k-Fold and Leave-One-Camera-Out cross-validation to estimate the accuracy of our model. In k-Fold cross-validation, the testing data is randomly split into k groups. It trains the model on k-1 of the groups and tests the model on the remaining group. Leave-One-Camera-Out cross-validation is virtually the same process but differs in the way the data is split up. In this method, the dataset is organized by which images were captured by a specific camera. Following the k-Fold logic, it then trains the model on all cameras' images except for one, and tests it on that last camera's images.</p>

<div style="clear:both">
<h3>The Active Learning Algorithm</h3>
	
	
<h2>Example of code with highlighting</h2>
The javascript in the <code>highlighting</code> folder is configured to do syntax highlighting in code blocks such as the one below.<p>

<pre><code>
%example code
for i = 1:length(offset)
    source = imread(sprintf('%s/source_%02d.jpg',data_dir,i));
    mask   = imread(sprintf('%s/mask_%02d.jpg',data_dir,i));
    target = imread(sprintf('%s/target_%02d.jpg',data_dir,i));

</code></pre>

<h3>Results</h3>
	
<p>The images of confustion matrices below track the evolution and improvement of our classification model as we refined our dataset and added features.</p>

<div style="float: center; padding: 20px">
<img src="Unprocessed_NaivePartition.png" />
<p style="font-size: 22px; text-align: center">Training and testing the base dataset with a naive partition provided very poor results with an average fscore of 0.2475.</p>
</div>

<div style="float: center; padding: 20px">
<img src="Cropped_NaivePartition.png" />
<p style="font-size: 22px; text-align: center">After running the dataset through the object detection script and cropping them down, there was some improvement, but still rather poor accuracy with an average fscore of 0.441 and 53.42% accuracy.</p>
</div>
	
<div style="float: center; padding: 20px">
<img src="Cropped_Compensated_1FoldPartition.png" />
<p style="font-size: 22px; text-align: center">In this test, we removed the nothing category of images as well as a few other categories that had very few images. To test accuracy, we implemented k-Fold cross-validation with a k value of 5, but used only one of the folds for training. This gave us an average fscore of .435 and our highest accuracy at 71.88%.</p>
</div>



<h3>Conclusions</h3>
<div style="clear:both" >
<p> 	While we weren't thrilled with the accuracy of our end product, we were still able to build a reasonably accurate animal classification model that incorporates Active Learning. Our most significant challenge was overcoming the flaws in our dataset. The vast disparity in number and quality of images for each category along with mislabling created a lot of issues in training our model that we were only able to partially remedy. Provided more time, we would have liked to find a balanced and well-curated dataset that we could train our model with and compare the results to what we got with our dataset. That being said, one of the exciting things about this model is its portability. Due to the properties of the Active Learning algorithm, the model can be quickly and easily deployed to new environments with minimal training data required. For example, the model could easily be trained on animal images from an African nature preserve that has much different animals from the ones at the Fairfield Osbourne Preserve in Sonoma County. It is a tool that could be used by ecologists to save time and energy and devote those precious resources elsewhere. </p>
</div>
    
<h3>References</h3>
<div style="clear:both" >
<p>
    <ul>
    <li><a href="DeepActiveLearning.pdf" target="_blank">A Deep Active Learning System for Species Identification and Counting in Camera Traps Images</a></li>
    <li><a href="https://github.com/microsoft/CameraTraps" target="_blank">Object Detection Script Github Page</a></li></ul></p>
</body>
</html>
